<head>
    <link rel="stylesheet"
          href="style.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/arta.min.css">

     <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/javascript.min.js"></script>
</head>
<script>hljs.highlightAll();</script>

<script type="module">
    import * as components from './scripts/components.js';
    window.BN = components.BN;
</script>

<body>
    <article>
        <h1>Infinite Compression</h1>
    </article>

    <article>
        <p>Infinite compression is a recursive compression scheme
           powered by off-the-shelf cryptography.
        </p>
        <p>
           While there is a minimum size data can be compressed into (it can't dissapear entirely),
           this compression can be run in as many recursive cycles as necessary to
           arrive at the desired size.
        </p>
        <p>As such, input data can be reduced to any target size above a minimum size
            no matter what the input data looks like <strong>or how large it is</strong> given
            enough cyles, so the <strong>maximium</strong> input size and type of data that can be compressed
            are both unbounded and able to arrive near the same minimum size no matter how
            much data is being compressed, it is thus called "infinite compression".
        </p>
        <p>
            The effectiveness of the compression algorithm decreases the closer to the
            minimum size the input data becomes, as the more data there is being compressed 
            the larger the distance between the frames, so larger payloads will have much
            higher compression rates than later cycles as the data gets smaller.
        </p>
    </article>
    <article>
        <h1>Encrypting</h1>
        <p>Data is first encrypted using a cypher, this results in a <italic>slightly</italic> larger
            but securely randomized data set.</p>
        <script id="encrypt-data-script">
async function encryptData(data) {
    if (typeof data === 'string') {
        // Convert text to ArrayBuffer
        const encoder = new TextEncoder();
        data = encoder.encode(payload);
    }

    // Generate a hash of the input data
    const hash = new Uint8Array(await window.crypto.subtle.digest('SHA-256', data));
    const key = hash.subarray(0, 32); // Use the first 32 bytes (256 bits) for the key

    // Also use for the initializing vector
    const iv = hash.subarray(0, 16);

    // Algorithm to use, AES-GCM is a good choice because it doesn't add much to the result
    const name = 'AES-GCM'

    // Import the encryption key
    const importedKey = await window.crypto.subtle.importKey(
        'raw', // format
        key,
        { name, length: 256 }, // algorithm
        false, // extractable
        ['encrypt'] // usage
    );

    // Encrypt the data
    const encryptedData = await window.crypto.subtle.encrypt(
        { name, iv },
        importedKey,
        data
    );

    return [key, encryptedData];
}
        </script>
        <code-highlighter script-id="encrypt-data-script"></code-highlighter>
        <p>Now we went from having one thing to having three things.
        </p>
        <p>We used to only have "data." Now we have newly encrypted "data,"
            a key to decrypt that data, and <strong>knowledge</strong>.
        </p>
        <p>What knowledge?</p>
        <p>Knowledge of the nature of the data. For instance, we know this
            data was encrypted and how to return it to its original form with
            the key.
        </p>
        <p>What is the role of this knowledge in infinite compression?</p>
        <p>Since this new data is the result of a cryptographic process, we know
            it is securely randomized.
        </p>
        <p>Our encrypted data is encoded as a sequence (of bytes). If we divide
            this sequence into 32-byte chunks (256 bits) we know that every number
            in this sequence will be probabilistically distributed evenly across
            that entire 256b address space.
        </p>
        <p>If we were to sort these into an array, each number would then be probabilistically
           equal in distance from the next.
        </p>
    </article>
    <article>
        <h1>Sorting</h1>
        <p>We treat the encrypted data as a sequence of very large numbers which we then
            sort along with their original position in the sequence so that the sequence
            can be re-constructed.</p>
        <script id="sort-data-script">
function sortData(encryptedData, key) {
    // Convert ArrayBuffer to Uint8Array
    const dataArray = new Uint8Array(encryptedData);

    // Split the array into 32-byte chunks (256 bits)
    const chunks = [];
    for (let i = 0; i < dataArray.length; i += 32) {
        chunks.push(dataArray.slice(i, i + 32));
    }

    // If the last chunk is not 32 bytes in length, fill the rest of the bytes with the key
    // this ensures a fully randomized distribution. if we left the bytes zero'd it may
    // result in an entry well outside the standard probability which would greatly
    // increase the space we need at the end to represent all the deltas.
    const lastChunk = chunks[chunks.length - 1];
    if (lastChunk.length < 32) {
        const remainingBytes = 32 - lastChunk.length;
        const keyBytes = key.slice(0, remainingBytes);
        chunks[chunks.length - 1] = new Uint8Array([...lastChunk, ...keyBytes]);
    }

    // Convert each chunk to a BN object and store the original position
    const bnChunks = chunks.map((chunk, index) => ({
        position: index,
        number: new BN(chunk)
    }));

    // Sort the BN objects
    bnChunks.sort((a, b) => a.number.cmp(b.number));

    return bnChunks;
}
        </script>
        <code-highlighter script-id="sort-data-script"></code-highlighter>
        <p>We have now aquired one new number for every 32 bytes (256b number) but 
            it's a small number because it starts at 0 and increases incrementally.
            This is a cost, but one that we can make up for in later compression.
            However, small payloads eventually hit a point at which we cannot acheive sufficient
            compression of the deltas to make up for this cost which is how we
            arrive at the minimum size. It's probabilistic, but not entirely knowable 
            until we try to compress it.
        </p>
    </article>

    <article>
        <h1>Deltas</h1>
        <p>Since this sequence is entirely ascending we can encode the delta from
            one entry to the next rather than the entire number.
        </p>
        
        <script id="convert-deltas-script">
function convertToDeltas(bnChunks) {
    let previousNumber = bnChunks[0].number;

    for (let i = 1; i < bnChunks.length; i++) {
        // Store the current number before it's overwritten
        const currentNumber = bnChunks[i].number;

        // Calculate the delta between the current number and the previous number
        const delta = currentNumber.sub(previousNumber);

        // Check if the delta is negative
        if (delta.isNeg()) {
            throw new Error('Delta should not be negative');
        }

        // Replace the current number with the delta
        bnChunks[i].number = delta;

        // Update the previous number for the next iteration
        previousNumber = currentNumber;
    }

    return bnChunks;
}
        </script>
        <script id="min-max-delta-script">
/* This is defined earlier than it is displayed because it's used in
    some early components below */
function findMinMaxDelta(bnChunks) {
    let minDelta = bnChunks[1].number;
    let maxDelta = bnChunks[1].number;

    for (let i = 2; i < bnChunks.length; i++) {
        if (bnChunks[i].number.lt(minDelta)) {
            minDelta = bnChunks[i].number;
        }
        if (bnChunks[i].number.gt(maxDelta)) {
            maxDelta = bnChunks[i].number;
        }
    }

    return { minDelta, maxDelta };
}
        </script>
        <p>We started with a sequence of very large numbers and now we have a sequence
            of smaller numbers that are <strong>not random.</strong>
        </p>
        <p>With larger payloads one may already acheive a space reduction that compensates
            for the additional smaller numbers representing offsets, but since we 
            now have a sequence of numbers that are <strong>not random</strong> we can compress them
            further with that knowledge.
        </p>
    </article>
    <article>
        <h1>Encode Smallest Possible Number Representation</h1>
        <p>
            By observing the high and low delta we gain knowledge of the number space
            that the deltas are distributed within. We can use this knowledge to reduce
            the encoded representation even further.
        </p>
        <p>One way to do this would be to simply remove the smallest value from every
            value since it's already being recorded. We would also be free at this
            point to reduce the space used by all numbers to the distance between the
            low and high delta as we know the highest possible value ahead of time.
        </p>
        <script id="remove-min-delta-script">
function removeMinDelta(bnChunks, minDelta) {
    // Subtract minDelta from every number in bnChunks
    const result = bnChunks.map(chunk => chunk.number.sub(minDelta));

    return result;
}
            </script>
        <code-highlighter script-id="min-max-delta-script"></code-highlighter>
        <code-highlighter script-id="remove-min-delta-script"></code-highlighter>

        <p>
            The laws of probability dictate that the average delta will be
            in half way point, or middle of this number space.
        </p>

        <h2>Sign Vector</h2>
        <p>We now have numbers that occur within a known fixed space that will group towards
           the middle of that space (probable average).
        </p>
        <p>
           We can calculate the middle point from already availabel information, then iterate 
           over the sequence and encode a single bit for each delta
           that indicates whether the delta is equal-or-above the probable average or below it.
        </p>
        <p>This vector will cost 1bit for every number in the sequence. The resultinng "sign vector" is 
            a new form of knowledge about the sequence that can be used to compress it further.
        </p>
        <p>The numbers can now be re-written as the distance between that number
           and the probable average.
        </p>
        <p>Note: it is more efficient to skip the removal of the minimum delta
           and the contraction of the number space and go straight to making
           the sign vector using the probable average instead of the middle of
           that new number space. These result in slightly different encodings
           that are probabilistically the same efficiency with less work. But,
           for the sake of this demonstration we will continue with the more laborious
           process as it is easier to understand.
        </p>

        <h1>Delta Compressed Pair Array Number Encoding</h1>
        <h3>Split</h3>
        <p>Data on the either of side of the probable average can be split
            into two arrays, one for above and one for below. The numbers
            will group near the probable average (which is now zero) resulting
            in much smaller numbers once again.
        </p>
        <h3>Sort</h3>
        <p>
            Since the numbers are large and group next to each other, we
            can convert each array to a pair array identical to what we did
            in the first step when we sorted the original encrypted data.
            This trades the cost of an additional small number (array positions
            increase incrementally from 0) for the ability to compress the new
            sequence again with a new round of delta compression.
        </p>
        <h3>Delta Compression</h3>
        <p>
            Once sorted, we can compress it down to the deltas as before
            but this time we'll acheive a much greater
            compression rate than the prior deltas because so many of the numbers will
            be close to each other.
        </p>
        <h3>Bit Level Number Encoding</h3>
        <p>Since the numbers group near the beginning of the number range
            the deltas will be much smaller in the beginning and will
            tend to get larger over time.</p>
        <p>
            Knowing this, we can record knowledge of the first occurance
            of each incease in the number size and all numbers from that
            point on will be encoded at that length. 
        </p>
        <p>
            We can then encode the numbers sequentially using the above defined
            bit lengths.
        </p>
<!-- 
        <script id="count-bitlengths-script">
function countBitLengths(bnChunks) {
    const bitLengthCounts = {};

    for (const chunk of bnChunks) {
        const bitLength = chunk.number.bitLength();

        if (bitLengthCounts[bitLength]) {
            bitLengthCounts[bitLength]++;
        } else {
            bitLengthCounts[bitLength] = 1;
        }
    }

    return bitLengthCounts;
}
        </script>
        <code-highlighter script-id="count-bitlengths-script"></code-highlighter>

        <script id="smallest-representation-script">
function smallestNumberSize(minDelta, maxDelta) {
    // Subtract minDelta from maxDelta
    const difference = maxDelta.sub(minDelta);

    // Return the bit length of the difference
    return difference.bitLength();
}
        </script>
        <code-highlighter script-id="smallest-representation-script"></code-highlighter>

<script>
function* readPackedNumbers(uint8Array, bitsPerNumber, numEntries) {
    let currentNumber = new BN(0);
    let bitsInCurrentNumber = 0;
    let entriesYielded = 0;

    for (let i = 0; i < uint8Array.length; i++) {
        for (let bit = 7; bit >= 0; bit--) {
            const bitValue = (uint8Array[i] >> bit) & 1;
            currentNumber = currentNumber.shln(1).or(new BN(bitValue));
            bitsInCurrentNumber++;

            if (bitsInCurrentNumber === bitsPerNumber) {
                yield currentNumber;
                currentNumber = new BN(0);
                bitsInCurrentNumber = 0;
                entriesYielded++;

                if (entriesYielded === numEntries) {
                    return;
                }
            }
        }
    }
}
</script>


    </article>
    <article>
        <h2>Simulation</h2>
        <script>
class FromElement extends HTMLElement {
    constructor() {
        super();
        this.attachShadow({ mode: 'open' });
    }

    connectedCallback() {
        const fromId = this.getAttribute('from-id');
        this.waitForElementAndResult(fromId).then(element => {
            this.setupUpdate(element);
        });
    }

    waitForElementAndResult(fromId) {
        return new Promise(resolve => {
            const checkElementAndResult = () => {
                const element = document.getElementById(fromId);
                if (element && element.value) {
                    resolve(element);
                } else {
                    requestAnimationFrame(checkElementAndResult);
                }
            };
            checkElementAndResult();
        });
    }

    setupUpdate(element) {
        // Update the component when the result property of the element changes
        new MutationObserver(() => this.update(element))
            .observe(element, { attributes: true });

        // Update the component when the value of a text input changes
        if (element.tagName === 'INPUT' && element.type === 'text') {
            element.addEventListener('input', () => this.update(element));
        }
        // Update the component when the 'resultChanged' event is dispatched
        element.addEventListener('resultChanged', () => this.update(element));

        // Initial update
        this.update(element);
    }

    // Utility function for high precision timers
    timeFunction(func, ...args) {
        const startTime = performance.now();
        const result = func(...args);
        const endTime = performance.now();
        const timeTaken = endTime - startTime;
        return { result, timeTaken };
    }

    // Utility function for high precision timers for async functions
    async timeAsyncFunction(asyncFunc, ...args) {
        const startTime = performance.now();
        const result = await asyncFunc(...args);
        const endTime = performance.now();
        const timeTaken = endTime - startTime;
        return { result, timeTaken };
    }

    // To be implemented by subclasses
    // update(result) {}
}

class EncryptDataFrom extends FromElement {
    async update(element) {
        const sizeInMB = parseInt(element.value)
        const sizeInBytes = sizeInMB * 1024 * 1024;

        // Create an empty buffer of the specified size
        const buffer = new ArrayBuffer(sizeInBytes);

        // Encrypt the buffer and measure the time taken
        const { result: [key, encryptedData], timeTaken } = await this.timeAsyncFunction(encryptData, buffer);
        this.value = { key, encryptedData }

        // Create elements to display the size of the key and the encrypted data
        this.shadowRoot.textContent = `encrypts to ${encryptedData.byteLength} bytes of data (or ${Math.ceil(encryptedData.byteLength / 32)} 256b frames) in ${timeTaken}ms`
        this.dispatchEvent(new CustomEvent('resultChanged'));
    }
}
customElements.define('encrypt-data-from', EncryptDataFrom);

class SortDataFrom extends FromElement {
    update(element) {
        const { encryptedData, key } = element.value;

        // Wrap the blocking line in requestAnimationFrame
        requestAnimationFrame(() => {
            // Sort the encrypted data and measure the time taken
            const { result: sortedData, timeTaken } = this.timeFunction(sortData, encryptedData, key);

            this.shadowRoot.textContent = `sorts in ${timeTaken}ms`
            this.value = sortedData;
            this.dispatchEvent(new CustomEvent('resultChanged'));
        });
    }
}
customElements.define('sort-data-from', SortDataFrom);
class EncDeltasFrom extends FromElement {
    update(element) {
        const value = element.value.map(obj => ({ ...obj }))

        // Wrap the blocking line in requestAnimationFrame
        requestAnimationFrame(async () => {
            // Convert the sorted data to deltas and measure the time taken
            const startTime = performance.now();

            const deltas = await convertToDeltas(value);
            const endTime = performance.now();
            const timeTaken = endTime - startTime;

            this.shadowRoot.textContent = `converts to deltas in ${timeTaken}ms`
            this.value = value;
            this.dispatchEvent(new CustomEvent('resultChanged'));
        });
    }
}
customElements.define('enc-deltas-from', EncDeltasFrom);
function createTableFromObject(obj) {
    // Create a table
    const table = document.createElement('table');

    // Create the header row
    const headerRow = document.createElement('tr');
    Object.keys(obj).forEach(key => {
        const th = document.createElement('th');
        th.textContent = key;
        headerRow.appendChild(th);
    });
    table.appendChild(headerRow);

    // Create the values row
    const valuesRow = document.createElement('tr');
    Object.values(obj).forEach(value => {
        const td = document.createElement('td');
        td.textContent = value;
        valuesRow.appendChild(td);
    });
    table.appendChild(valuesRow);

    return table;
}
function roundByte (bits) {
    return Math.ceil(bits / 8)
}
class FrameViewer extends FromElement {
    update(element) {
        // Clear the shadow root
        this.shadowRoot.innerHTML = '';

        const frames = element.value
        const length = frames.length
        
        const offsetSize = roundByte(new BN(length).bitLength());
        const result = { frames: length, offsetSize }
        // Check if every entry in the array is greater than the prior entry
        const isIncreasing = frames.slice(1).every((frame, i) => frame.number.gt(frames[i].number));

        if (!isIncreasing) {
            const { maxDelta, minDelta } = findMinMaxDelta(frames);
            result.deltaSize = roundByte(maxDelta.bitLength());
            result.largestDistanceSize = roundByte(smallestNumberSize(minDelta, maxDelta))

            // Count the bit lengths
            const bitLengthCounts = countBitLengths(frames);

            // Create a table for the deltas
            const deltasTable = createTableFromObject(bitLengthCounts);
            this.shadowRoot.appendChild(deltasTable);
        }
        result.firstFrameSize = roundByte(frames[0].number.bitLength())

        const table = createTableFromObject(result);

        // Add the table to the shadow root
        this.shadowRoot.appendChild(table);
        // Create the scatter plot button element
        const scatterPlotButton = document.createElement('scatter-plot-button');

        // Set the from-id attribute
        scatterPlotButton.setAttribute('from-id', this.id);

        // Append the element to the shadow root
        this.shadowRoot.appendChild(scatterPlotButton);

        this.frames = frames
        this.value = result
        this.dispatchEvent(new CustomEvent('resultChanged'));
    }
}
customElements.define('frame-viewer', FrameViewer);
class ScatterPlotButton extends FromElement {
    constructor() {
        super();
    }

    update(element) {
        this.value = element   
        this.shadowRoot.innerHTML = `
            <button id="generate">plot deltas</button>
        `;
        this.shadowRoot.querySelector('#generate').addEventListener('click', this.generateScatterplot.bind(this));

        // Create and append the script element separately
        const script = document.createElement('script');
        script.src = "https://cdn.jsdelivr.net/npm/chart.js";
        this.shadowRoot.appendChild(script);
    }
    generateScatterplot() {
        const element = this.value
        const MAX_SAFE_INTEGER = new BN(Number.MAX_SAFE_INTEGER);
        const { maxDelta } = findMinMaxDelta(element.frames);
        let LARGE_CONSTANT = maxDelta;

        while (LARGE_CONSTANT.cmp(MAX_SAFE_INTEGER) > 0) {
            LARGE_CONSTANT = LARGE_CONSTANT.div(new BN(2));
        }

        this.value = element.frames.slice(1).map(({ number, position }) => {
            let scaledNumber = number;
            while (scaledNumber.cmp(LARGE_CONSTANT) >= 0) {
                scaledNumber = scaledNumber.div(LARGE_CONSTANT);
            }
            return { y: scaledNumber.toNumber(), x: position };
        });

        // Defer the heavy work
        requestAnimationFrame(() => {
            this.shadowRoot.removeChild(this.shadowRoot.querySelector('#generate'))
            const canvas = document.createElement('canvas');
            canvas.id = 'scatterplot';
            this.shadowRoot.appendChild(canvas);
            const ctx = canvas.getContext('2d');
            new Chart(ctx, {
                type: 'scatter',
                data: {
                    datasets: [{
                        label: 'Delta Plot',
                        data: this.value
                    }]
                },
                options: {
                    scales: {
                        x: { type: 'linear', position: 'bottom' }
                    }
                }
            });
        }, 0);
    }
}
customElements.define('scatter-plot-button', ScatterPlotButton);
        </script>
        <form>
            <p>Data that is <input id="demo1-input" value="1"></input>megabytes,</p>
            <p>Then <encrypt-data-from id="demo1-enc" from-id="demo1-input"></encrypt-data-from>,</p>
            <p>Which <sort-data-from id="demo1-sort" from-id="demo1-enc"></sort-data-from>,</p>
            <frame-viewer id="demo1-first-frames" from-id="demo1-sort"></frame-viewer>
            <p>Which <enc-deltas-from id="demo1-delta1" from-id="demo1-sort"></enc-deltas-from>,</p>
            <frame-viewer id="demo1-second-frames" from-id="demo1-delta1"></frame-viewer>
            <delta-viewer id="demo1-delta-viewer" from-id="demo1-delta1"></delta-viewer>
            <p>Since </p>
        </form>
    </article>
    
        <code-highlighter script-id="smallest-representation-script"></code-highlighter>

    </article>
    <article>
        <h1>Recursive Encoding</h1>
        <p>
            As long as the final encoding is smaller than the original data set
            the process can be repeated by encrypting again, and this process
            can be recursively encoded and unpacked by encoding the number of
            recursions into every final encoding.
        </p>
    </article> -->

</body>